# Research Paper: RNN part

## Introduction to RNN 

Recurrent Neural Networks are a type of neural networks which are known to
perform better in time series prediction. Unlike other types of neural networks,
RNNs benefit from an internal memory. To explain it with an example, to predict
the next word in a sentence, a normal neural network would take the last word as
an input and would miss the sentence context, whereas an RNN would take a
sequence of words, for instance the last 10 words, which would give a better
understanding of the context, and it would predict the next word with much more
accuracy. 

This applies perfectly to time series prediction, as the next value in a time
series will highly depend on the trend before the value that needs to be imputed. 

## Our implementation of RNN 

In this research two architectures of RNNs on time series prediction were
compared: LSTM and GRU. They both use long and short-term memory, but in a
slightly different way, their performance can differ. In this case, GRU
outperformed LSTM. 

The two methods were compared using a genetic algorithm. Random RNN
configurations would be generated by changing the architecture (GRU or LSTM),
the number (1-5) and size (2-100) of hidden layers, the size of the input
sequence (2-12), as well as the loss function (MSE or Huber). 

The best configuration we found using this method used GRU, 1 hidden layer of
size 95, an input sequence of 12 data points, and the MSE loss function. 

The final model was composed of a GRU layer with the above configuration,
followed by a fully connected layer, transforming the GRU layer output into a
prediction. For each data column that was imputed, there had to be a trained
model. 

## Limitations of implementation 

Our implementation had two major limitations: 
- First it predicts only one value at a time, based on the X preceding values.
This means that for imputing a gap of size above one, it will at some point
make predictions based on previously predicted values. This can result in an
important bias on large gaps, as a prediction error will have an impact on all
following predictions. 
- Second, it is trained using an error metric (Mean Squared Error in this case).
However, ultimately the goal was to predict the trends more than the values
themselves. If we predict correctly a water consumption spike at the wrong
moment in the day, I might be better than not predicting it at all. Yet, an
error-based metric will prefer not predicting it at all rather than predicting
it at the wrong time. 

## Proposed solutions 

We propose a solution to the two previously stated limitations. Using a
sequence-to-sequence model, such as an encoder-decoder RNN would consist of
predicting a sequence of values, given an input sequence. For instance, by
taking the 100 values before a gap, the model could predict the following 100
values, which we would use to fill the gap. This way, our prediction would be
based on real values only, and we could use another training metric, such as
variance over the predicted sequence compared to the variance over the original
sequence. 

## Resources

[https://www.nature.com/articles/s41598-018-24271-9.pdf](https://www.nature.com/articles/s41598-018-24271-9.pdf) 
[https://www.sciencedirect.com/science/article/pii/S0378778819333717](https://www.sciencedirect.com/science/article/pii/S0378778819333717)
[https://proceedings.neurips.cc/paper/2018/file/734e6bfcd358e25ac1db0a4241b95651-Paper.pdf](https://proceedings.neurips.cc/paper/2018/file/734e6bfcd358e25ac1db0a4241b95651-Paper.pdf)
[https://machinelearningmastery.com/how-to-develop-lstm-models-for-multi-step-time-series-forecasting-of-household-power-consumption/](https://machinelearningmastery.com/how-to-develop-lstm-models-for-multi-step-time-series-forecasting-of-household-power-consumption/)
[http://faculty.marshall.usc.edu/gareth-james/Research/bv.pdf](http://faculty.marshall.usc.edu/gareth-james/Research/bv.pdf)
[https://builtin.com/data-science/recurrent-neural-networks-and-lstm](https://builtin.com/data-science/recurrent-neural-networks-and-lstm)
